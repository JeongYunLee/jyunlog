{"parsed":{"_path":"/blog/2024-01-16-llama2","_dir":"blog","_draft":false,"_partial":false,"_locale":"","_empty":false,"title":"LLaMA 2 Open Foundation and Fine-Tuned Chat Models ë…¼ë¬¸ ë¦¬ë·°","description":"í•´ë‹¹ ê¸€ì€ 'LLaMA 2: Open Foundation and Fine-Tuned Chat Models' ë…¼ë¬¸ ë¦¬ë·°ì´ë©°, Udemyì˜ 'ëª¨ë‘ë¥¼ ìœ„í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ LLM Part 1Section3: LLaMA2 ëª¨ë¸ ë¦¬ë·°' ê°•ì˜ë¥¼ ì ê·¹ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.","date":"2024-01-16","tags":["LLM","LLaMA2","paper review","NLP"],"draft":false,"summary":"2023ë…„ 7ì›” ê³µê°œëœ LLaMA2 ë…¼ë¬¸ ë¦¬ë·° ê¸€ ì…ë‹ˆë‹¤.","body":{"type":"root","children":[{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"í•´ë‹¹ ê¸€ì€ "},{"type":"element","tag":"i","props":{},"children":[{"type":"text","value":"'"},{"type":"element","tag":"a","props":{"href":"https://arxiv.org/abs/2307.09288","rel":["nofollow"]},"children":[{"type":"text","value":"LLaMA 2: Open Foundation and Fine-Tuned Chat Models"}]},{"type":"text","value":"'"}]},{"type":"text","value":" ë…¼ë¬¸ ë¦¬ë·°ì´ë©°, Udemyì˜ '"},{"type":"element","tag":"a","props":{"href":"https://www.udemy.com/course/llm-part-1-LLaMA-2-fine-tuning/","rel":["nofollow"]},"children":[{"type":"text","value":"ëª¨ë‘ë¥¼ ìœ„í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ LLM Part 1"},{"type":"element","tag":"span","props":{},"children":[{"type":"text","value":"Section3: LLaMA2 ëª¨ë¸ ë¦¬ë·°"}]}]},{"type":"text","value":"' ê°•ì˜ë¥¼ ì ê·¹ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"2023ë…„ 7ì›”ì— ğŸ’«ê°™ì´ ë“±ì¥í•œ LLaMA2ëŠ” Transformer ê¸°ë°˜ì˜ LLMìœ¼ë¡œ, LLaMA1 ì¶œì‹œ í›„ 5ê°œì›” ë§Œì— ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ì´ë¤„ëƒˆìŠµë‹ˆë‹¤. LLaMAê°€ í° ì£¼ëª©ì„ ë°›ì€ ì´ìœ ëŠ” ë°”ë¡œ ëª¨ë¸ì˜ ê°€ë²¼ì›€ ëŒ€ë¹„ ë›°ì–´ë‚œ ì„±ëŠ¥ê³¼ ëˆ„êµ¬ë‚˜ ìƒì—…ì  ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë„ë¡ í•œ ì˜¤í”ˆì†ŒìŠ¤ ì •ì±… ë•Œë¬¸ì…ë‹ˆë‹¤. í•™ìŠµê³¼ êµ¬ë™í•  ë•Œì˜ compute budgetì„ ê³ ë ¤í•˜ì—¬ GPT3 175B(1750ì–µ íŒŒë¼ë¯¸í„°)ì— ë¹„í•´ 1/10 ì´ìƒ ì‚¬ì´ì¦ˆë¥¼ ì¶•ì†Œì‹œì¼°ìœ¼ë©°, ì„±ëŠ¥ í‰ê°€ë¥¼ í–ˆì„ ë•Œ ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ê³  í•©ë‹ˆë‹¤. ë˜í•œ LLaMA 6.7B ëª¨ë¸ì€ ë‹¨ì¼ì„œë²„(V100)ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥í•˜ì—¬ ë³´ë‹¤ ì ‘ê·¼ì„± ìˆëŠ” ëª¨ë¸ì´ë¼ê³ ë„ ë°í˜”ìŠµë‹ˆë‹¤. ë¬´ì œí•œìœ¼ë¡œ ëŠ˜ì–´ê°€ëŠ” ëª¨ë¸ì˜ í¬ê¸°ì™€ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ê´€ë ¨í•´ì„œ ëŠ˜ ì´ìŠˆê°€ ìˆì—ˆë˜ LLM ì‹œì¥ì— ë°˜ê°€ìš´ ëª¨ë¸ì´ ë“±ì¥í•œ ê²ƒì´ì£ ."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"LLaMA2 ë…¼ë¬¸ì€ ì´ 8íŒŒíŠ¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤."}]},{"type":"element","tag":"ol","props":{"start":0},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Abstract"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Introduction"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Pre-Training"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Fine-Tuning"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Safety"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Discussion"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Related Work"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Conclusion"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"ì•„ë˜ ê·¸ë¦¼ì€ chat ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” Pre-Trainingê³¼ Fine-Tuning ê³¼ì •ì— ëŒ€í•´ ì „ì²´ì ìœ¼ë¡œ ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤. ë³¸ ë¦¬ë·°ì—ì„œëŠ” ì´ í•™ìŠµ ìˆœì„œì— ë”°ë¼ì„œ í•˜ë‚˜ì”© ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."}]},{"type":"element","tag":"figure","props":{},"children":[{"type":"text","value":"\n    "},{"type":"element","tag":"img","props":{"src":"/images/blogImg/2024-01-16-1.png","title":"LLaMA2-process"},"children":[]},{"type":"text","value":"    \n    "},{"type":"element","tag":"figcaption","props":{"style":"text-align: center;"},"children":[{"type":"text","value":"LLaMA2ì˜ pre-trainingê³¼ fine-tuning ê³¼ì •"}]}]},{"type":"element","tag":"h2","props":{"id":"_1-pre-training"},"children":[{"type":"text","value":"1. Pre-Training"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"LLaMA2ì˜ Pre-Trainingì€ LLaMA1ê³¼ ê±°ì˜ ìœ ì‚¬í•˜ê²Œ ì§„í–‰ë˜ì–´ í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ì´ì „ ë²„ì „ê³¼ ì°¨ì´ì ì— ì´ˆì ì„ ë‘ì–´ ì„¤ëª…í•©ë‹ˆë‹¤."}]},{"type":"element","tag":"ol","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"ë” ë§ì€ í•™ìŠµ ë°ì´í„° ì‚¬ìš©\ní•™ìŠµ ë°ì´í„°ëŠ” ì¸í„°ë„·ì—ì„œ ê³µê°œì ìœ¼ë¡œ ìˆ˜ì§‘í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë¥¼ ì‚¬ìš©í–ˆê³ , Metaì˜ ë°ì´í„°(Instagram, Facebook ë“±ì˜ ê°œì¸ ì •ë³´)ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤ê³  í•©ë‹ˆë‹¤. LLaMA1ì— ë¹„í•´ 40% ì¦ê°€í•œ 2ì¡°ê°œì˜ í•™ìŠµ í† í°ì„ ì‚¬ìš©í–ˆëŠ”ë°, ì´ë•Œ í† í°ì˜ ì˜ë¯¸ëŠ” ë§ë­‰ì¹˜(corpus)ì˜ ê¸¸ì´ê°€ 2ì¡°ê°œë¼ëŠ” ì˜ë¯¸ê°€ ì•„ë‹Œ, ì „ì²´ epochì—ì„œ ì‚¬ìš©í•œ í† í°ì„ ë‹¤ í•©ì¹œ ê°’ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í•™ìŠµ epochê°€ 2ì¼ ë•Œ, í•™ìŠµ corpusëŠ” 1ì¡°ê°œê°€ ë˜ëŠ”ê±°ì£ .\ní•œ ê°€ì§€ íŠ¹ì´í•œ ì ì€, í•™ìŠµ ë°ì´í„°ì—ì„œ ëª¨ë“  ìœ í•´ ì»¨í…ì¸ ë¥¼ í•„í„°ë§ í•˜ì§„ ì•Šì•˜ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ì˜ í¸í–¥ì„±ê³¼ ìœ í•´ì„±ì„ ì´ìœ ë¡œ í•™ìŠµ ë°ì´í„°ì…‹ì„ robustí•˜ê²Œ ì •ì œí•˜ëŠ” ê²½í–¥ì´ ìˆëŠ”ë° LLaMAì˜ ê²½ìš° ëª¨ë¸ì´ ê¸°ë³¸ì ìœ¼ë¡œ ìœ í•´í•œ ë‚´ìš©ë„ ì „ë¶€ í•™ìŠµí•˜ê²Œ í•œ ë’¤, Fine-Tuning ê³¼ì •ì—ì„œ í•´ë‹¹ ë‚´ìš©ì€ ìœ í•´í•˜ë‹¤ëŠ” ê²ƒì„ ë‹¤ì‹œ í•™ìŠµì‹œì¼œì£¼ëŠ” ë°©ì‹ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤. ë˜í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ì˜ ë°ì´í„°ëŠ” ì—…ìƒ˜í”Œë§í•˜ì—¬ ë” ë§ì´ í•™ìŠµë˜ë„ë¡ ë§Œë“¤ì–´ì„œ hallucinationì„ ìµœëŒ€í•œ ì¤„ì´ë„ë¡ ë…¸ë ¥í–ˆë‹¤ê³  í•©ë‹ˆë‹¤.\ní•™ìŠµ ë°ì´í„°ì˜ ì–¸ì–´ëŠ” ë‹¤ì–‘í•˜ì§€ë§Œ, ì˜ì–´ê°€ 89.7%ë¥¼ ì°¨ì§€í•˜ê³  í•œêµ­ì–´ëŠ” 0.06%ê°€ëŸ‰ ë°–ì— ì—†ì–´ì„œ ì‹¤ì œë¡œ LLaMAë¥¼ ì‚¬ìš©í•´ë³´ë©´ í•œêµ­ì–´ ì„±ëŠ¥ì´ ë§¤ìš° ë–¨ì–´ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"context ê¸¸ì´ë¥¼ 2ë°°ë¡œ\nLLaMA2ëŠ” ê¸°ì¡´ì˜ context ê¸¸ì´ì¸ 2048ì˜ 2ë°°ì¸ 4096ì…ë‹ˆë‹¤. context ê¸¸ì´ëŠ” í•œ ë²ˆì˜ inputìœ¼ë¡œ ë„£ì–´ ì£¼ëŠ” ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. contextì˜ ê¸¸ì´ê°€ ê¸¸ìˆ˜ë¡ ì•ë’¤ ë¬¸ë§¥ íŒŒì•…ê³¼ í¬í•¨í•  ìˆ˜ ìˆëŠ” ì •ë³´ì˜ ì–‘ì€ ë§ì•„ì ¸ ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ë§Œ, í•™ìŠµì€ ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Grouped Query Attention (GQA)\nGQAëŠ” êµ¬ê¸€ì—ì„œ ê°œë°œí•œ ê¸°ìˆ ë¡œ, MHA(Multi-Head Attention)ì™€ MQA(Multi-Query Attention)ì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬ ì¶”ë¡  ì†ë„ë¥¼ ë¹ ë¥´ê²Œ í•˜ë©´ì„œ ì„±ëŠ¥ì€ ìœ ì§€í•  ìˆ˜ ìˆëŠ” ë°©ì‹ì…ë‹ˆë‹¤."}]}]},{"type":"element","tag":"figure","props":{},"children":[{"type":"text","value":"\n    "},{"type":"element","tag":"img","props":{"src":"/images/blogImg/2024-01-16-2.png","title":"compare with GQA"},"children":[]},{"type":"text","value":"    \n    "},{"type":"element","tag":"figcaption","props":{"style":"text-align: center;"},"children":[{"type":"element","tag":"a","props":{"href":"https://arxiv.org/pdf/2305.13245.pdf"},"children":[{"type":"text","value":"GQAì™€ MHA, MQA ë¹„êµ"}]}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"GQAëŠ” Multi-headì™€ Mul-Queryì˜ ì¤‘ê°„ì  ë°©ë²•ìœ¼ë¡œ, MHAë³´ë‹¤ ë””ì½”ë”© ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ë©° í•˜ë©° ë™ì‹œì— MQAì˜ í•™ìŠµì˜ ë¶ˆì•ˆì •ì„±ì„ ë³´ì™„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤. Queryì˜ ê·¸ë£¹ì„ ë§Œë“¤ê³  ê° ê·¸ë£¹ë§ˆë‹¤ Keyì™€ Valueë¥¼ ë‘ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. GQAëŠ” ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì—ë„ ì ìš©í•  ìˆ˜ ìˆìœ¼ë©°(pre-trainedëœ ëª¨ë¸ì— ì¶”ê°€ë¡œ GQAë¡œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•) LLaMA2ì—ì„œëŠ” 70Bëª¨ë¸ì—ì„œë§Œ ì ìš©í•˜ì—¬ ì„±ëŠ¥ì´ ë›°ì–´ë‚œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."}]},{"type":"element","tag":"blockquote","props":{},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"MHAì™€ MQAë€?"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Multi-Head Attentionì€ Attention scoreë¥¼ ê³„ì‚°í•  ë•Œ Query, Key, Valueë¥¼ ê°ê° headì˜ ìˆ˜ ë§Œí¼ ë³‘ë ¬ë¡œ ë‚˜ëˆ„ì–´ ê³„ì‚°ì„ í•œ ë’¤, ë§ˆì§€ë§‰ì— concatenateí•˜ì—¬ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ë¶€ë¶„(head)ì—ì„œ ê²°ê³¼ë¥¼ ë‚˜ëˆ„ì–´ ë„ì¶œí•˜ì—¬ ì„œë¡œ ìƒí˜¸ë³´ì™„í•˜ê¸° ë•Œë¬¸ì— ê¸°ì¡´ì— ë¹„í•´ ì„±ëŠ¥ì´ í–¥ìƒë  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.\nMulti-Query-Attentionì€ Queryì— ëŒ€í•´ì„œëŠ” ì›ë˜ í—¤ë“œ ìˆ˜ë¥¼ ìœ ì§€í•˜ì§€ë§Œ Keyì™€ Valueì— ëŒ€í•œ í—¤ë“œëŠ” í•˜ë‚˜ë§Œ ê°–ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ë°©ì‹ì€ ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë””ì½”ë”ì—ì„œ ìƒë‹¹í•œ ì†ë„ í–¥ìƒì„ ì´ë£° ìˆ˜ ìˆìœ¼ë‚˜ ì„±ëŠ¥ì €í•˜ì˜ ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"GQAì™€ ê´€ë ¨ëœ ìì„¸í•œ ë‚´ìš©ì€ ë‹¤ìŒ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì„¸ìš”."}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"a","props":{"href":"https://arxiv.org/abs/2305.13245","rel":["nofollow"]},"children":[{"type":"text","value":"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}]}]}]},{"type":"element","tag":"h2","props":{"id":"_2-fine-tuning-supervised-fine-tuning-sft"},"children":[{"type":"text","value":"2. Fine-Tuning: Supervised Fine-Tuning (SFT)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"SFTëŠ” ì…ë ¥ promptê°’ê³¼ ì¶œë ¥ response ê°’ ìŒì„ í™œìš©í•˜ì—¬ í•™ìŠµí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. LLaMA2ì—ì„œëŠ” SFT ê³¼ì •ì—ì„œëŠ” ë°ì´í„°ì˜ ì–‘ë³´ë‹¤ ì§ˆì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ì„ í–‰ì—°êµ¬(LIMA:) ì— ë”°ë¼ì„œ ìì²´ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìƒì‚°í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. ì•½ 27540ê°œì˜ ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ ì§ì ‘ ìƒì‚°í•˜ì—¬ í•™ìŠµí–ˆì„ ë•Œ, ê¸°ì¡´ì— êµ¬í•  ìˆ˜ ìˆëŠ” ìˆ˜ë°±ë§Œê°œì˜ ì €í’ˆì§ˆ ë°ì´í„°ë¡œ í•™ìŠµí•˜ì˜€ì„ ë•Œ ë³´ë‹¤ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"êµ¬ì¶•í•œ promptì™€ responseë°ì´í„° ìŒì€ ëª¨ë¸ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì ì ˆíˆ ë§ì¶”ì–´ í•™ìŠµ íš¨ê³¼ë¥¼ ë†’ì´ê¸° ìœ„í•´ (ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ì´ ë“¤ì–´ê°€ë©´ ë¬¸ë§¥íŒŒì•…ì´ ì–´ë ¤ì›Œ í•™ìŠµ íš¨ê³¼ê°€ ë‚®ê¸° ë•Œë¬¸) "},{"type":"element","tag":"code-inline","props":{},"children":[{"type":"text","value":"prompt<SEG>response"}]},{"type":"text","value":"ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ concatenate í•´ì„œ í•™ìŠµì˜ inputìœ¼ë¡œ ë„£ì–´ì¤ë‹ˆë‹¤. ì´í›„ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í† í°ì— ëŒ€í•œ lossë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ê²°ê³¼ì ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë¯€ë¡œ ì‹¤ì œ ì •ë‹µê°’ì— ëŒ€í•´ì„œë§Œ backpropagationì„ ì§„í–‰í•©ë‹ˆë‹¤."}]},{"type":"element","tag":"blockquote","props":{},"children":[{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"'Auto-regressive'ë€? âœ… (p9)"},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nìê¸° ìì‹ ì„ ì…ë ¥ìœ¼ë¡œ í•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ì˜ˆì¸¡í•˜ëŠ” ëª¨í˜•. í˜„ì¬ ì‹œì ê¹Œì§€ ìƒì„±í•œ ouputì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì‹œì ì˜ outputì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"SFTì™€ ê´€ë ¨ëœ ì¶”ê°€ì ì¸ í•™ìŠµì€ ë‹¤ìŒ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì„¸ìš”"}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"a","props":{"href":"https://arxiv.org/abs/2212.10560","rel":["nofollow"]},"children":[{"type":"text","value":"SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions"}]},{"type":"text","value":": modelì´ ìƒì„±í•œ prompt-responseìŒìœ¼ë¡œ SFT ì§„í–‰í•˜ëŠ” ë‚´ìš©"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"a","props":{"href":"https://arxiv.org/abs/2305.11206%29","rel":["nofollow"]},"children":[{"type":"text","value":"LIMA: Less Is More for Alignment"}]},{"type":"text","value":": ì–‘ì§ˆì˜ ì ì€ ë°ì´í„°ë¡œ í•™ìŠµí–ˆì„ ë•Œ ì„±ëŠ¥ì´ ë” ë›°ì–´ë‚˜ë‹¤ëŠ” ë‚´ìš©"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"a","props":{"href":"https://openreview.net/forum?id=kce6LTZ5vY","rel":["nofollow"]},"children":[{"type":"text","value":"Instruction Mining: Instruction Data Selection for Tuning Large Language Models"}]},{"type":"text","value":": SFT ë°ì´í„°ì…‹ í’ˆì§ˆ ì¸¡ì • ê´€ë ¨ ë‚´ìš©"}]}]},{"type":"element","tag":"h2","props":{"id":"_3-fine-tuning-mainaligned-model"},"children":[{"type":"text","value":"3. Fine-Tuning: Main(Aligned) Model"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"LLaMA2ëŠ” pretrainingë¶€í„° SFTë¥¼ ì§€ë‚˜ëŠ” chatì„ ìœ„í•œ ì „ì²´ì ì¸ main(aligned) modelê³¼ fine-tuningì„ ìœ„í•œ reward ë‘ê°œì˜ ëª¨ë¸ì´ ì¡´ì¬í•©ë‹ˆë‹¤. (ë‘ ëª¨ë¸ í•™ìŠµì˜ ìˆœì„œëŠ” ì„¤ëª… í¸ì˜ìƒ ë¶€ì—¬í•œ ê²ƒì´ë©° ë…¼ë¬¸ ìƒ ì •í™•í•œ ì–¸ê¸‰ì€ ì—†ìŠµë‹ˆë‹¤) SFTë¥¼ ì§„í–‰í•œ ë’¤ ìƒì„±ëœ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ê° prompt ë‹¹ Kê°œì˜ responseë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. ì´í›„ . ì´ ì¶œë ¥ê°’ ê°ê°ì— ëŒ€í•´ reward modelì„ í†µí•´ì„œ reward scoreë¥¼ ìƒì„±í•˜ê³  ì´ ì¤‘ ê°€ì¥ ë†’ì€ ê°’ì„ ê°–ëŠ” responseë¥¼ ì„ íƒí•˜ëŠ” ê³¼ì •ì´ Reject Samplingì…ë‹ˆë‹¤."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"ë‹¤ìŒ 4ë‹¨ê³„ì—ì„œëŠ” ìœ„ ê³¼ì •ì—ì„œ í•„ìš”í•œ Reward Modelì„ ë§Œë“œëŠ” ë°©ë²•ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤."}]},{"type":"element","tag":"h2","props":{"id":"_4-fine-tuning-reward-model"},"children":[{"type":"text","value":"4. Fine-Tuning: Reward Model"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"ì´ˆê¸° reward ëª¨ë¸ì€ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œëœ Human Preference Datasetê³¼ Metaì—ì„œ êµ¬ì¶•í•œ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤."}]},{"type":"element","tag":"ol","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"helpfulness: meta helpfulness50%, meta safety25%, open source 25%"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"safety: 90%ëŠ” meta + Anthropic Harmless, 10%ëŠ” meta safety"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"temperature íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì ˆí•˜ì—¬ ë‘ê°œì˜ ëª¨ë¸ì„ ë§Œë“¤ê³  ê° ëª¨ë¸ì— ë™ì¼í•œ promptë¥¼ ë„£ì–´ì„œ responseë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. ì´í›„ ì‚¬ëŒì€ ë‘ê°œì˜ ì¶œë ¥ê°’ì— ëŒ€í•˜ì—¬ safetyì™€ helpfulness ë‘ ê°€ì§€ ê´€ì ì— ë”°ë¼ì„œ responseë¥¼ í‰ê°€í•˜ê²Œ ë©ë‹ˆë‹¤. í‰ê°€ ì„ ì§€ëŠ” significantly better, better, slightly better, negligibly better ë„¤ ë‹¨ê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ë“¤ì–´, 'ì˜¤ëŠ˜ì€ ë¹„ê°€ì˜¤ëŠ”ë°, ì €ë…ì„ ì¶”ì²œí•´ì¤˜'ë¼ëŠ” promptì— ëŒ€í•˜ì—¬ modelAëŠ” 'ë¹„ê°€ ì˜¤ëŠ” ë‚ ì—ëŠ” ë”°ëœ»í•œ êµ­ë¬¼ ìš”ë¦¬ê°€ ì¢‹ê² ì–´ìš”. ë¼ë©´ ì–´ë– ì‹ ê°€ìš”?' ë¼ëŠ” ë‹µë³€ì„ í•˜ê³  modelBëŠ” 'ê¹€ë°¥ì„ ì¶”ì²œí•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì¬ë£Œê°€ ë“¤ì–´ê°€ ê±´ê°•ì— ì¢‹ì€ ìŒì‹ì´ì—ìš”.' ë¼ëŠ” ë‹µë³€ì„ í–ˆë‹¤ë©´ safety ê´€ì ì—ì„œëŠ” ë‘ ë‹µë³€ ëª¨ë‘ ì•ˆì „ì„± ì¸¡ë©´ì— ì°¨ì´ê°€ ì—†ìœ¼ë¯€ë¡œ negligibly betterì„ ì„ íƒí•  ìˆ˜ ìˆê³ , helpfulness ê´€ì ì—ì„œëŠ” modelAì˜ ë‹µë³€ì´ ë” ì ì ˆí•˜ì§€ë§Œ mobelBì˜ ë‹µë³€ë„ í‹€ë¦¬ì§„ ì•Šê¸° ë•Œë¬¸ì— better ì •ë„ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LLaMA2ëŠ” ì•ˆì „ì„±ì„ ê°•ì¡°í•˜ê¸° ë•Œë¬¸ì— safety ê´€ì ì—ì„œëŠ” í•œ ê°€ì§€ í‰ê°€ë¥¼ ë” ì§„í–‰í•©ë‹ˆë‹¤. ë™ì¼í•˜ê²Œ ì¶œë ¥ëœ ë‘ responseë¥¼ 3ê°€ì§€ í‰ê°€ ì„ ì§€(one is safe but the other is unsafe, both safe, both unsafe)ì— ëŒ€í•˜ì—¬ í‰ê°€í•˜ê³  both unsafeë¼ëŠ” ë‹µë³€ì„ ë°›ì€ reponseëŠ” í•™ìŠµ ë°ì´í„°ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.\nhuman annotatorsì˜ helpfulnessì™€ safety ê°ê°ì˜ ì´ì§„ ë¶„ë¥˜ ë¼ë²¨ì— ë”°ë¼ì„œ pretrainedê³¼ SFTë¥¼ ë§ˆì¹œ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì´ì§„ë¶„ë¥˜ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. ì´ í•™ìŠµì€ pretrainingì˜ í•™ìŠµ ë°©ì‹ê³¼ ë™ì¼í•˜ì§€ë§Œ, ë‹¤ìŒ í† í° ì˜ˆì¸¡ì„ ìœ„í•œ ë¶„ë¥˜ í—¤ë“œê°€ scalar reward scoreë¥¼ ì¶œë ¥í•˜ê¸° ìœ„í•œ regression í—¤ë“œë¡œ ëŒ€ì²´ëœë‹¤ëŠ” ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. ì¶œë ¥ëœ chosen scalar scoreì™€ Rejected scalar scoreë¥¼ í™œìš©í•˜ì—¬ ê¸°ì¡´ì— 4ë‹¨ê³„ë¥¼ ë‚˜ëˆ  ë‹µë³€ì„ ë°›ì€ ê²ƒì— ëŒ€í•œ marginì„ ë¶€ì—¬í•˜ì—¬ ranking lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. marginì€ ë‘ ë¬¸ì¥ ì‚¬ì´ì˜ ê±°ë¦¬, ì¦‰ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ì— ëŒ€í•œ ê°’ì´ë¼ì„œ ë‘ ë¬¸ì¥ì˜ ì°¨ì´ê°€ í¬ì§€ ì•Šë‹¤ë©´ marginì€ ì‘ìŠµë‹ˆë‹¤. ì¦‰, modelì´ confidenceë¥¼ ì«Œ ë” í™•ì‹¤í•˜ê²Œ ê°€ì ¸ê°€ê²Œ í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ì„œ ë„ì…í•œ ê²ƒ ì…ë‹ˆë‹¤. ì•„ë˜ í‘œì—ì„œ marginì„ ê³ ë ¤í–ˆì„ ë•Œ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."}]},{"type":"element","tag":"figure","props":{},"children":[{"type":"text","value":"\n    "},{"type":"element","tag":"img","props":{"src":"/images/blogImg/2024-01-16-3.png","title":"reward model margin"},"children":[]},{"type":"text","value":"    \n    "},{"type":"element","tag":"figcaption","props":{"style":"text-align: center;"},"children":[{"type":"text","value":"marginì— ë”°ë¥¸ model accuracy ì°¨ì´"}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Meta helpfulness and safety dataëŠ” reward modelingì„ ìœ„í•´ì„œ êµ¬ì¶•í•˜ì—¬ domainì  í•œê³„ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë‚˜, 100ë§Œê°œ ì´ìƒì˜ ì´ì§„ë¹„êµë¥¼ ì‹¤ì‹œí–ˆê³  ì „ì²´ ë°ì´í„° ìˆ˜, ëŒ€í™”ë‹¹ í‰ê·  í„´ ìˆ˜, ì˜ˆì‹œë‹¹ í‰ê·  í† í° ìˆ˜, í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µë‹¹ í‰ê·  í† í° ìˆ˜ ë“±ì—ì„œ íƒ€ ë°ì´í„° ì…‹ì— ë¹„í•´ ìš°ìˆ˜í•˜ë‹¤ê³  í•©ë‹ˆë‹¤."}]},{"type":"element","tag":"figure","props":{},"children":[{"type":"text","value":"\n    "},{"type":"element","tag":"img","props":{"src":"/images/blogImg/2024-01-16-4.png","title":"mata helpfulness and safety data"},"children":[]},{"type":"text","value":"    \n    "},{"type":"element","tag":"figcaption","props":{"style":"text-align: center;"},"children":[{"type":"text","value":"Metaì—ì„œ êµ¬ì¶•í•œ helpfulness and safety data í†µê³„"}]}]},{"type":"element","tag":"h2","props":{"id":"_5-reinforce-learning-human-feedback-rlhf"},"children":[{"type":"text","value":"5. Reinforce Learning Human Feedback (RLHF)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Kê°œì˜ reward scoreë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ 4ì—ì„œ ìƒì„±í•œ Reward Modelì— Kê°œì˜ responseë¥¼ ë„£ì–´ì¤ë‹ˆë‹¤."}]},{"type":"element","tag":"h2","props":{"id":"_6-rlhf-reject-sampling"},"children":[{"type":"text","value":"6. RLHF: Reject Sampling"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"prompt ë‹¹ ì¶œë ¥ëœ Kê°œì˜ reward scoreì€ safety reward scoreì™€ helpfulness reward score ê°ê° ìƒì„±ë©ë‹ˆë‹¤. ì´ ì¤‘ ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë‹µë³€ë§Œ sampling í•´ì£¼ëŠ” ë‹¨ê³„ê°€ Reject Samplingì…ë‹ˆë‹¤."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"ì´ ê³¼ì •ì€ 70B ëª¨ë¸ì˜ ê²½ìš°ì—ë§Œ ì‹¤í–‰í•˜ê³  ì´ ë³´ë‹¤ ì‘ì€ ëª¨ë¸ì˜ ê²½ìš° 70Bëª¨ë¸ì˜ ìƒì„±ê²°ê³¼ë¥¼ SFT ë°©ì‹ìœ¼ë¡œ distillation(ê°€ì¥ í° ëª¨ë¸ì„ í†µí•´ì„œ ë„ì¶œëœ ê²°ê³¼ê°’ì„ ì‘ì€ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•œë‹¤ëŠ” ì˜ë¯¸ë¡œ í•´ì„) í•´ì¤€ë‹¤ê³  í•©ë‹ˆë‹¤. (distillationì˜ ì„±ëŠ¥ê³¼ ê´€ë ¨ëœ ì—°êµ¬ëŠ” í–¥í›„ í…ŒìŠ¤í¬ë¡œ ë‘ )"}]},{"type":"element","tag":"h2","props":{"id":"_7-rlhf-iterative-fine-tuning"},"children":[{"type":"text","value":"7. RLHF: Iterative Fine-Tuning"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"ê¸°ì¡´ì˜ RLHFëŠ” ì„ í˜•ì ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ë°˜ë©´, LLaMA2ì—ì„œëŠ” ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ SFTì—ì„œ Reward Modelì˜ ê²°ê³¼ë¥¼ ì¬í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. 6ë²ˆì—ì„œ samplingí•´ì¤€ ë°ì´í„°ë¥¼ ë‹¤ì‹œ SFT í•™ìŠµ ë°ì´í„°ë¡œ ë„£ì–´ì¤˜ì„œ 2~6ê¹Œì§€ì˜ í•™ìŠµ ê³¼ì •ì„ ë°˜ë³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. LLaMA2ì—ì„œëŠ” ì´ ë°˜ë³µì„ 5ë²ˆ í•´ì¤¬ë‹¤ê³  í•©ë‹ˆë‹¤."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"í•™ìŠµì„ ë°˜ë³µí•˜ì—¬ ì§„í–‰í–ˆì„ ë•Œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ì§€ì— ëŒ€í•œ íŒë‹¨ì€ Model-basedì™€ Human Evaluation ë‘ ê°€ì§€ë¡œ ì§„í–‰í•˜ì—¬ íŒë‹¨í•©ë‹ˆë‹¤. ìš°ì„  Model-based Evaluationì€ í•™ìŠµì„ ë°˜ë³µì„ í•  ìˆ˜ë¡ reward modelì˜ ê°’ì´ í–¥ìƒë˜ëŠ” ì§€ì— ëŒ€í•´ í™•ì¸í•˜ê¸° ìœ„í•´ reward modelì˜ scoreê³¼ 3ëª…ì˜ annotatorê°€ ì±…ì •í•œ ì ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤. safetyì™€ helpfulnessì— ëŒ€í•˜ì—¬ ê°ê° 1586ê°œ, 584ê°œì˜ ë³„ë„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìƒì„±í•´ì„œ ì§„í–‰í–ˆì„ ë•Œ ì•„ë˜ ê·¸ë˜í”„ì—ì„œì™€ ê°™ì´ ì–‘ì˜ ìƒê´€ê´€ê³„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"},{"type":"element","tag":"span","props":{},"children":[{"type":"text","value":"fig 29"}]},{"type":"text","value":"\nì´ë ‡ê²Œ ê²€ì¦í•œ reward modelì„ ì‚¬ìš©í•˜ì—¬ ë°˜ë³µí•œ RLHF V1~V5ê¹Œì§€ì˜ ê²°ê³¼ëŠ” ChatGPTì™€ì˜ ìƒëŒ€ ë¹„êµë¥¼ í†µí•´ í™•ì¸í•˜ëŠ”ë°, helpfulnessì™€ safetyëŠ” ëª¨ë‘ itertationì„ í•  ìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤.\në‘ ë²ˆì§¸ëŠ” Human Evaluationì…ë‹ˆë‹¤. 4000ê°œì˜ single, multi turn dialogë¥¼ ì¤€ë¹„í•˜ê³  modelë¡œë¶€í„° responseë¥¼ ìƒì„±í•˜ë„ë¡ í•œ ë’¤ 3ëª…ì˜ annotatorê°€ ì±…ì •í•œ ì ìˆ˜ë¥¼ í™•ì¸í•´ë³´ë©´ ChatGPTì™€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ê²°ê³¼ë¥¼ ì–»ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. (ì´ëŸ¬í•œ í‰ê°€ ë°©ë²•ì— ëŒ€í•œ í•œê³„ì ì´ ìˆìŒì„ ì–¸ê¸‰í•˜ê¸°ë„ í•©ë‹ˆë‹¤)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"ê¸°ì¡´ì˜ RLHF í•™ìŠµ ë°©ë²•ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì€ ì•„ë˜ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì„¸ìš”."}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"a","props":{"href":"https://arxiv.org/abs/2203.02155","rel":["nofollow"]},"children":[{"type":"text","value":"Training language models to follow instructions with human feedback"}]},{"type":"text","value":": InstructGPT, RLHF ê´€ë ¨ ë‚´ìš©"}]}]},{"type":"element","tag":"h2","props":{"id":"_8-rlhf-proximal-policy-optimization"},"children":[{"type":"text","value":"8. RLHF: Proximal Policy Optimization"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Reject Samplingì„ 5ë²ˆ í•´ì¤€ ë’¤, ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œëŠ” anootated ëœ ì‚¬ëŒì˜ reward scoreë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì í™”í•˜ëŠ” PPOë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ì´ë•Œ ì‚¬ìš©í•˜ëŠ” rewardëŠ” ì¶œë ¥ëœ safety rewardì™€ helpfulness reward ì¤‘ í•˜ë‚˜ì˜ rewardë§Œ ì„ íƒí•´ì£¼ëŠ” ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤. ë‘ rewardì˜ ê°’ì„ ë¹„êµí–ˆì„ ë•Œ, safety rewardì˜ ê°’ì´ 0.15ë³´ë‹¤ í¬ë©´ helpfulness rewardë¥¼ ì„ íƒí•˜ì—¬ ì •í™•ë„ë¥¼ ë”ìš± ë†’ì´ê³ , ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° safety rewardë¥¼ ì„ íƒí•˜ì—¬ ì•ˆì „ì„±ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•˜ë„ë¡ í•©ë‹ˆë‹¤."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Proximal Policy Optimizationì— ëŒ€í•´ì„œ ì •í™•íˆ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ê°•í™”í•™ìŠµ ì „ë°˜ì— ëŒ€í•´ ì´í•´ê°€ í•„ìš”í•©ë‹ˆë‹¤. PPOì™€ ê´€ë ¨ëœ ì„¸ë¶€ì ì¸ ë‚´ìš©ì€ ì•„ë˜ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì„¸ìš”."}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"a","props":{"href":"https://arxiv.org/abs/1707.06347","rel":["nofollow"]},"children":[{"type":"text","value":"Proximal Policy Optimization Algorithms"}]}]}]},{"type":"element","tag":"h2","props":{"id":"_9-ghost-attention-gatt"},"children":[{"type":"text","value":"9. Ghost Attention (GATT)"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Ghost Attentionì€ LLaMA2ì˜ contributionì— í•´ë‹¹ë˜ëŠ” ê¸°ìˆ ë¡œ, Multi-turn ì±„íŒ… ìƒí™©ì—ì„œ Instructionì´ ì§€ì†ë˜ë„ë¡ í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ë•Œ Instructionì€ ë…¼ë¬¸ ì˜ˆë¥¼ ë”°ë¥´ë©´, 'ì§ˆë¬¸ì— ëŒ€í•´ ì´ëª¨í‹°ì½˜ìœ¼ë¡œë§Œ ë‹µí•´ì¤˜' ë¼ëŠ” ì§€ì‹œê°€ ìˆìœ¼ë©´ promptì™€ responseê°€ ê³„ì† ì§€ì†ë˜ì–´ë„ ëê¹Œì§€ ë¬¸ìë¡œ ë‹µí•˜ì§€ ì•Šê³  ì´ëª¨í‹°ì½˜ìœ¼ë¡œ ë‹µí•˜ë„ë¡ ìœ ì§€í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. Ghost Attentionì˜ ë°©ë²•ì€ Context distillationì—ì„œ ì°©ì•ˆí•˜ì—¬ ê°œë°œí•˜ì˜€ê³  ë°ì´í„° ìƒì„± ê³¼ì •ì—ì„œë§Œ instructionì„ userì˜ ì…ë ¥ë§ˆë‹¤ ì‚½ì…í•´ì„œ supervised fine-tuning ê³¼ì •ì—ì„œ ì§„í–‰í•˜ê²Œ ë©ë‹ˆë‹¤."}]},{"type":"element","tag":"h2","props":{"id":"ì°¸ê³ ìë£Œ"},"children":[{"type":"text","value":"ì°¸ê³ ìë£Œ"}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"span","props":{},"children":[{"type":"text","value":"ë¦¬ë·°"}]},{"type":"text","value":" Meta AIì˜ Small Gaint Model: LLaMA(Large Language Model Meta AI), daewoo kim ["},{"type":"element","tag":"a","props":{"href":"https://moon-walker.medium.com/%EB%A6%AC%EB%B7%B0-meta-ai%EC%9D%98-small-gaint-model-LLaMA-large-language-model-meta-ai-334e349ed06f","rel":["nofollow"]},"children":[{"type":"text","value":"ë°”ë¡œê°€ê¸°"}]},{"type":"text","value":"]"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"ì˜¤í”ˆì†ŒìŠ¤ LLMì˜ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜: Meta AIì˜ LLaMA2 â€” (1) overview, daewoo kim ["},{"type":"element","tag":"a","props":{"href":"https://moon-walker.medium.com/%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4-llm%EC%9D%98-%ED%8C%A8%EB%9F%AC%EB%8B%A4%EC%9E%84-%EC%A0%84%ED%99%98-meta-ai%EC%9D%98-LLaMA2-1-overview-2412764787cb","rel":["nofollow"]},"children":[{"type":"text","value":"ë°”ë¡œê°€ê¸°"}]},{"type":"text","value":"]"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"span","props":{},"children":[{"type":"text","value":"ë¦¬ë·°"}]},{"type":"text","value":" Meta AIì˜ Small Gaint Model: LLaMA(Large Language Model Meta AI), daewoo kim ["},{"type":"element","tag":"a","props":{"href":"https://moon-walker.medium.com/%EB%A6%AC%EB%B7%B0-meta-ai%EC%9D%98-small-gaint-model-LLaMA-large-language-model-meta-ai-334e349ed06f","rel":["nofollow"]},"children":[{"type":"text","value":"ë°”ë¡œê°€ê¸°"}]},{"type":"text","value":"]"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"span","props":{},"children":[{"type":"text","value":"ë¦¬ë·°"}]},{"type":"text","value":" Meta AIì˜ ë…¼ë¬¸ LIMA(Less Is More for Alignment): ê²°êµ­ LLMì˜ Pre-trainingì´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤?, daweoo kim ["},{"type":"element","tag":"a","props":{"href":"https://moon-walker.medium.com/%EB%A6%AC%EB%B7%B0-meta-ai%EC%9D%98-%EB%85%BC%EB%AC%B8-lima-less-is-more-for-alignment-%EA%B2%B0%EA%B5%AD-llm%EC%9D%98-pre-training%EC%9D%B4-%EA%B0%80%EC%9E%A5-%EC%A4%91%EC%9A%94%ED%95%98%EB%8B%A4-f3c9ea885f5a","rel":["nofollow"]},"children":[{"type":"text","value":"ë°”ë¡œê°€ê¸°"}]},{"type":"text","value":"]"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"span","props":{},"children":[{"type":"text","value":"ë…¼ë¬¸ ë¦¬ë·°"}]},{"type":"text","value":" LLaMA2, DAJE ["},{"type":"element","tag":"a","props":{"href":"https://dajeblog.co.kr/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-LLaMA2/","rel":["nofollow"]},"children":[{"type":"text","value":"ë°”ë¡ê°€ê¸°"}]},{"type":"text","value":"]"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"span","props":{},"children":[{"type":"text","value":"Paper Review"}]},{"type":"text","value":" LLaMA 2: Open Foundation and Fine-Tuned Chat Models, Jaehee Kim ["},{"type":"element","tag":"a","props":{"href":"http://dsba.korea.ac.kr/seminar/?mod=document&uid=2738","rel":["nofollow"]},"children":[{"type":"text","value":"ë°”ë¡œê°€ê¸°"}]},{"type":"text","value":"]"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"span","props":{},"children":[{"type":"text","value":"ì¸ê³µì§€ëŠ¥,ë¨¸ì‹ ëŸ¬ë‹,ë”¥ëŸ¬ë‹"}]},{"type":"text","value":" (ì‹¬í™”) LLaMA2, ì»´ë‹¬ì¸ ["},{"type":"element","tag":"a","props":{"href":"https://www.youtube.com/watch?v=iGpughMIwH0","rel":["nofollow"]},"children":[{"type":"text","value":"ë°”ë¡œê°€ê¸°"}]},{"type":"text","value":"]"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"span","props":{},"children":[{"type":"text","value":"ì¸ê³µì§€ëŠ¥,ë¨¸ì‹ ëŸ¬ë‹,ë”¥ëŸ¬ë‹"}]},{"type":"text","value":" (ì‹¬í™”) LLaMA2 - Ghost Attention, ì»´ë‹¬ì¸ ["},{"type":"element","tag":"a","props":{"href":"https://www.youtube.com/watch?v=fwt0tgYtWUc","rel":["nofollow"]},"children":[{"type":"text","value":"ë°”ë¡œê°€ê¸°"}]},{"type":"text","value":"]"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"ë¼ë§ˆ2ì— ì ìš©ëœ ì¶”ë¡  ì†ë„ í–¥ìƒ ê¸°ìˆ ì¸ GQA(Grouped Query Attention)ì— ëŒ€í•´, singleheart ["},{"type":"element","tag":"a","props":{"href":"https://devocean.sk.com/blog/techBoardDetail.do?ID=165192","rel":["nofollow"]},"children":[{"type":"text","value":"ë°”ë¡œê°€ê¸°"}]},{"type":"text","value":"]"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Multi-Query Attention Explained, Florian ["},{"type":"element","tag":"a","props":{"href":"https://pub.towardsai.net/multi-query-attention-explained-844dfc4935bf","rel":["nofollow"]},"children":[{"type":"text","value":"ë°”ë¡œê°€ê¸°"}]},{"type":"text","value":"]"}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Auto Regressive Models, ratsgo's blog ["},{"type":"element","tag":"a","props":{"href":"https://ratsgo.github.io/generative%20model/2018/01/31/AR/","rel":["nofollow"]},"children":[{"type":"text","value":"ë°”ë¡œê°€ê¸°"}]},{"type":"text","value":"]"}]}]}],"toc":{"title":"","searchDepth":2,"depth":2,"links":[{"id":"_1-pre-training","depth":2,"text":"1. Pre-Training"},{"id":"_2-fine-tuning-supervised-fine-tuning-sft","depth":2,"text":"2. Fine-Tuning: Supervised Fine-Tuning (SFT)"},{"id":"_3-fine-tuning-mainaligned-model","depth":2,"text":"3. Fine-Tuning: Main(Aligned) Model"},{"id":"_4-fine-tuning-reward-model","depth":2,"text":"4. Fine-Tuning: Reward Model"},{"id":"_5-reinforce-learning-human-feedback-rlhf","depth":2,"text":"5. Reinforce Learning Human Feedback (RLHF)"},{"id":"_6-rlhf-reject-sampling","depth":2,"text":"6. RLHF: Reject Sampling"},{"id":"_7-rlhf-iterative-fine-tuning","depth":2,"text":"7. RLHF: Iterative Fine-Tuning"},{"id":"_8-rlhf-proximal-policy-optimization","depth":2,"text":"8. RLHF: Proximal Policy Optimization"},{"id":"_9-ghost-attention-gatt","depth":2,"text":"9. Ghost Attention (GATT)"},{"id":"ì°¸ê³ ìë£Œ","depth":2,"text":"ì°¸ê³ ìë£Œ"}]}},"_type":"markdown","_id":"content:blog:2024-01-16-llama2.md","_source":"content","_file":"blog/2024-01-16-llama2.md","_extension":"md"},"hash":"fgQGLius2R"}